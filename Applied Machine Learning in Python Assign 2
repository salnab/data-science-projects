def answer_one():
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.pipeline import make_pipeline
    import numpy as np

    x_t = np.linspace(0,10,100)
    predlist = []
    
    for i in [1,3,6,9]:
        model = make_pipeline(PolynomialFeatures(i), LinearRegression())
        model = model.fit(X_train[:, np.newaxis], y_train)
        predlist.append(model.predict(x_t[:, np.newaxis]))
    return np.array(predlist)


def answer_two():
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.metrics.regression import r2_score
    from sklearn.pipeline import make_pipeline

    x_t = np.linspace(0,10,100)
    R2score_training = []
    R2score_test = []
    
    for i in range(0,10):
        model = make_pipeline(PolynomialFeatures(i), LinearRegression())
        model = model.fit(X_train[:, np.newaxis], y_train)
        pred_train = model.predict(X_train[:, np.newaxis])
        pred_test = model.predict(X_test[:, np.newaxis])
        R2score_training.append(r2_score(pred_train, y_train))
        R2score_test.append(r2_score(pred_test, y_test))
    return np.array(R2score_training), np.array(R2score_test)


def answer_three():
    
    y = []
    y = answer_two()
    y_train = y[0].tolist()
    y_test = y[1].tolist()
    degree = range(0,10)
    
    #import matplotlib.pyplot as plt
    #%matplotlib notebook
    #plt.figure(figsize=(10,5))
    #plt.plot(degree, y_train, 'o', label='training data', markersize=10)
    #plt.plot(degree, y_test, 'o', label='test data', markersize=10)
    #plt.ylim(-8,1.5)
    #plt.legend(loc=4)
    
    r2train_max = max(y_train)
    Overfitting = y_train.index(r2train_max)
    r2train_min = min(y_train)
    Underfitting_train = y_train.index(r2train_min)
    r2test_min = min(y_test)
    Underfitting = y_test.index(r2test_min)
    r2test_max = max(y_test)
    Good_Generalization = y_test.index(r2test_max)
    
    return Underfitting, Overfitting, Good_Generalization


def answer_four():
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import Lasso, LinearRegression
    from sklearn.metrics.regression import r2_score
    from sklearn.pipeline import make_pipeline

    X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)
    linreg = LinearRegression().fit(X_train.reshape(-1,1), y_train)
    linreg_pred = linreg.predict(X_test.reshape(-1,1))
    LinearRegression_R2_test_score = r2_score(linreg_pred, y_test)
    
    model = make_pipeline(PolynomialFeatures(12), Lasso(alpha=0.01, max_iter=10000))    
    model = model.fit(X_train.reshape(-1,1), y_train)
    pred_test = model.predict(X_test.reshape(-1,1))
    Lasso_R2_test_score = r2_score(pred_test, y_test)
    
    return LinearRegression_R2_test_score, Lasso_R2_test_score


def answer_five():
    from sklearn.tree import DecisionTreeClassifier
    
    clf = DecisionTreeClassifier().fit(X_train2, y_train2)
    features = X_train2.columns.values.tolist()
    feat_imp = clf.feature_importances_.tolist()
    #feat_imp_sort = sorted(feat_imp, reverse=True)
    feat_imp_index = [b[0] for b in sorted(enumerate(feat_imp), key=lambda i:i[1], reverse=True)]
    #feat_imp_index = sorted(range(len(feat_imp)), key=lambda k: feat_imp[k], reverse=True)
    feat_imp_sort = [features[i] for i in feat_imp_index]
    return feat_imp_sort[:5]
    
    
def answer_six():
    from sklearn.svm import SVC
    from sklearn.model_selection import validation_curve
    import numpy as np

    clf = SVC(kernel = 'rbf', random_state=0)
    param_range = np.logspace(-4,1,6)
    training_scores, test_scores = validation_curve(clf, X_subset, y_subset, param_name='gamma', param_range=param_range, scoring='accuracy')
    training_scores = np.mean(training_scores, axis=1)
    test_scores = np.mean(test_scores, axis=1)
    return training_scores, test_scores
    
    
def answer_seven():
    y = []
    y = answer_six()
    y_train = y[0].tolist()
    y_test = y[1].tolist()
    gamma = np.logspace(-4,1,6)
    
    #import matplotlib.pyplot as plt
    #%matplotlib notebook
    #plt.figure(figsize=(10,5))
    #plt.plot(np.log10(gamma), y_train, 'o', label='training data', markersize=10)
    #plt.plot(np.log10(gamma), y_test, 'o', label='test data', markersize=10)
    #plt.ylim(0,1.5)
    #plt.legend(loc=4)
    
    train_min = min(y_train)
    train_min_index = y_train.index(train_min)
    Underfitting = gamma[train_min_index]
    test_min = min(y_test)
    test_min_index = y_test.index(test_min)
    Overfitting = gamma[test_min_index]
    test_max = max(y_test)
    test_max_index = y_test.index(test_max)
    Good_Generalization = gamma[test_max_index]
    
    return Underfitting, Overfitting, Good_Generalization
